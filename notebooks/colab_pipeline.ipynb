{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Clinical Handoff Summary Pipeline\n",
    "\n",
    "EMR 데이터 로드 → 소견 추출 → 해석 (중복 제거/충돌 해소) → 인수인계 요약 생성\n",
    "\n",
    "이 노트북은 Google Colab에서 실행할 수 있도록 설계되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/taejun-song/emr-icu-handover.git\n",
    "%cd emr-icu-handover\n",
    "!pip install -q anthropic pandas openpyxl pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "from src.config import (\n",
    "    BASELINE_FILE, INPUT_DATA_FILE, OUTPUT_DIR, EXTRACTIONS_DIR,\n",
    "    OUTPUT_FRAMEWORK_FILE, SHEET_NAMES, SHEET_NAME_TO_PROMPT,\n",
    ")\n",
    "from src.loader import load_emr_file\n",
    "from src.extractors import extract_all, extract_sheet\n",
    "from src.interpreter import interpret\n",
    "from src.synthesizer import synthesize\n",
    "from src.schemas import ExtractorOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load EMR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sheets = load_emr_file(BASELINE_FILE)\n",
    "data_sheets = load_emr_file(INPUT_DATA_FILE)\n",
    "\n",
    "print(f\"Baseline sheets: {list(baseline_sheets.keys())}\")\n",
    "print(f\"Data sheets: {list(data_sheets.keys())}\")\n",
    "for name, df in data_sheets.items():\n",
    "    print(f\"  {name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_outputs = await extract_all(data_sheets)\n",
    "\n",
    "print(f\"Extracted from {len(extractor_outputs)} sheets:\")\n",
    "for eo in extractor_outputs:\n",
    "    print(f\"  {eo.sheet_name}: {len(eo.findings)} findings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for eo in extractor_outputs:\n",
    "    out_path = EXTRACTIONS_DIR / f\"{eo.sheet_name.replace(' ', '_').lower()}.json\"\n",
    "    out_path.write_text(json.dumps(eo.model_dump(), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {eo.sheet_name: eo for eo in extractor_outputs}\n",
    "\n",
    "for sheet_name, eo in results.items():\n",
    "    source_rows = len(data_sheets[sheet_name])\n",
    "    findings_count = len(eo.findings)\n",
    "    header = (\n",
    "        f\"### {sheet_name}\\n\\n\"\n",
    "        f\"**Source rows:** {source_rows} | \"\n",
    "        f\"**Findings extracted:** {findings_count}\\n\\n\"\n",
    "    )\n",
    "    rows = []\n",
    "    for i, f in enumerate(eo.findings, 1):\n",
    "        rows.append(f\"| {i} | {f.datetime or '—'} | {f.category} | {f.content[:120]}{'…' if len(f.content) > 120 else ''} |\")\n",
    "    table = \"| # | Datetime | Category | Content |\\n|---|----------|----------|---------|\\n\" + \"\\n\".join(rows)\n",
    "    display(Markdown(header + table))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rows = []\n",
    "for sheet_name in SHEET_NAMES:\n",
    "    source_rows = len(data_sheets.get(sheet_name, pd.DataFrame()))\n",
    "    findings = len(results[sheet_name].findings) if sheet_name in results else 0\n",
    "    pct = (findings / source_rows * 100) if source_rows > 0 else 0.0\n",
    "    coverage_rows.append({\"Sheet\": sheet_name, \"Source Rows\": source_rows, \"Findings Extracted\": findings, \"Coverage %\": round(pct, 1)})\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_output = await interpret(extractor_outputs, baseline_sheets)\n",
    "\n",
    "print(f\"Reconciled findings: {len(interpreter_output.reconciled_findings)}\")\n",
    "print(f\"Conflicts resolved:  {len(interpreter_output.conflicts_resolved)}\")\n",
    "print(f\"Duplicates removed:  {interpreter_output.duplicates_removed}\")\n",
    "print(f\"Input findings:      {interpreter_output.metadata.total_input_findings}\")\n",
    "print(f\"Output findings:     {interpreter_output.metadata.total_output_findings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "interp_path = OUTPUT_DIR / \"interpretation.json\"\n",
    "interp_path.write_text(\n",
    "    json.dumps(interpreter_output.model_dump(), ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(f\"Saved: {interp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, rf in enumerate(interpreter_output.reconciled_findings, 1):\n",
    "    sources = \", \".join(rf.sources)\n",
    "    note = rf.resolution_note or \"—\"\n",
    "    content_preview = rf.content[:100] + (\"…\" if len(rf.content) > 100 else \"\")\n",
    "    rows.append(f\"| {i} | {rf.datetime or '—'} | {content_preview} | {sources} | {note} |\")\n",
    "\n",
    "table = (\n",
    "    \"| # | Datetime | Content | Sources | Resolution Note |\\n\"\n",
    "    \"|---|----------|---------|---------|--------------------|\\n\"\n",
    "    + \"\\n\".join(rows)\n",
    ")\n",
    "display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflict Resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interpreter_output.conflicts_resolved:\n",
    "    display(Markdown(\"*No conflicts detected.*\"))\n",
    "else:\n",
    "    for i, cr in enumerate(interpreter_output.conflicts_resolved, 1):\n",
    "        md = (\n",
    "            f\"### Conflict {i}\\n\\n\"\n",
    "            f\"**Description:** {cr.description}\\n\\n\"\n",
    "            f\"**Sources:** {', '.join(cr.sources)}\\n\\n\"\n",
    "            f\"**Resolution:** {cr.resolution}\"\n",
    "        )\n",
    "        display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_output = await synthesize(interpreter_output)\n",
    "\n",
    "print(f\"Findings incorporated: {synthesizer_output.metadata.findings_incorporated}\")\n",
    "print(f\"Date range: {synthesizer_output.metadata.date_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Synthesis Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_sheets = pd.read_excel(OUTPUT_FRAMEWORK_FILE, engine=\"openpyxl\", sheet_name=None)\n",
    "expected_sections = list(framework_sheets.keys())\n",
    "\n",
    "summary_text = synthesizer_output.summary.lower()\n",
    "coverage_rows = []\n",
    "for section in expected_sections:\n",
    "    found = section.lower() in summary_text\n",
    "    coverage_rows.append({\"Section\": section, \"Found in Summary\": found})\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "found_count = coverage_df[\"Found in Summary\"].sum()\n",
    "total = len(coverage_df)\n",
    "print(f\"Section coverage: {found_count}/{total} ({found_count/total*100:.0f}%)\\n\")\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(synthesizer_output.summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = OUTPUT_DIR / \"summary.md\"\n",
    "summary_path.write_text(synthesizer_output.summary, encoding=\"utf-8\")\n",
    "print(f\"Saved: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}