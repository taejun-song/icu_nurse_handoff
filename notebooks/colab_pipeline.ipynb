{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Clinical Handoff Summary Pipeline\n",
    "\n",
    "EMR 데이터 로드 → 소견 추출 → 해석 (중복 제거/충돌 해소) → 인수인계 요약 생성\n",
    "\n",
    "이 노트북은 Google Colab에서 실행할 수 있도록 설계되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Install Dependencies\n!git clone https://github.com/taejun-song/emr-icu-handover.git\n%cd emr-icu-handover\n!pip install -q huggingface_hub pandas openpyxl pydantic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Set API Key\nimport os\nfrom google.colab import userdata\nos.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Imports\nimport json\nimport pandas as pd\nfrom pathlib import Path\nfrom IPython.display import Markdown, display\nfrom src.config import (\n    BASELINE_FILE, INPUT_DATA_FILE, OUTPUT_DIR, EXTRACTIONS_DIR,\n    OUTPUT_FRAMEWORK_FILE, SHEET_NAMES, SHEET_NAME_TO_PROMPT,\n)\nfrom src.loader import load_emr_file\nfrom src.extractors import extract_all, extract_sheet\nfrom src.interpreter import interpret\nfrom src.synthesizer import synthesize\nfrom src.schemas import ExtractorOutput"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load EMR Data\n\n아래 셀을 실행하면 파일 업로드 위젯이 나타납니다.\n각 슬롯에 해당하는 Excel 파일을 드래그 앤 드롭하거나 클릭하여 업로드하세요."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import src.config as _cfg\nfrom google.colab import output\nfrom IPython.display import HTML, display\nimport base64, asyncio\n\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\nFILE_SLOTS = {\n    \"baseline\": {\"label\": \"Input_Baseline\", \"desc\": \"환자 기본 정보\", \"config\": \"BASELINE_FILE\"},\n    \"data\": {\"label\": \"Input_Data\", \"desc\": \"EMR 데이터\", \"config\": \"INPUT_DATA_FILE\"},\n    \"framework\": {\"label\": \"Output_Framework\", \"desc\": \"출력 프레임워크\", \"config\": \"OUTPUT_FRAMEWORK_FILE\"},\n}\n_uploaded = {}\n\ndef _receive_file(slot_id, filename, b64content):\n    content = base64.b64decode(b64content)\n    dest = DATA_DIR / filename\n    dest.write_bytes(content)\n    _uploaded[slot_id] = dest\n    return f\"{filename} ({len(content)/1024:.1f} KB)\"\n\noutput.register_callback(\"notebook._receive_file\", _receive_file)\n\ndisplay(HTML(\"\"\"\n<style>\n.upload-container { display: flex; gap: 12px; flex-wrap: wrap; margin: 8px 0; }\n.upload-slot {\n  flex: 1; min-width: 180px; border: 2px dashed #ccc; border-radius: 8px;\n  padding: 16px; text-align: center; cursor: pointer; transition: all 0.2s;\n  background: #fafafa; position: relative;\n}\n.upload-slot:hover, .upload-slot.dragover { border-color: #4285f4; background: #e8f0fe; }\n.upload-slot.done { border-color: #34a853; border-style: solid; background: #e6f4ea; }\n.upload-slot .label { font-weight: bold; font-size: 14px; margin-bottom: 4px; }\n.upload-slot .desc { font-size: 12px; color: #666; }\n.upload-slot .status { font-size: 12px; margin-top: 8px; color: #888; }\n.upload-slot.done .status { color: #34a853; font-weight: bold; }\n.upload-slot input { display: none; }\n</style>\n<div class=\"upload-container\" id=\"upload-container\"></div>\n<script>\nconst slots = {\n  baseline:  {label: \"Input_Baseline\", desc: \"환자 기본 정보\"},\n  data:      {label: \"Input_Data\", desc: \"EMR 데이터\"},\n  framework: {label: \"Output_Framework\", desc: \"출력 프레임워크\"}\n};\nconst container = document.getElementById(\"upload-container\");\nObject.entries(slots).forEach(([id, s]) => {\n  const div = document.createElement(\"div\");\n  div.className = \"upload-slot\";\n  div.id = \"slot-\" + id;\n  div.innerHTML = `\n    <div class=\"label\">${s.label}</div>\n    <div class=\"desc\">${s.desc}</div>\n    <div class=\"status\" id=\"status-${id}\">클릭 또는 드래그</div>\n    <input type=\"file\" id=\"input-${id}\" accept=\".xlsx\">`;\n  div.addEventListener(\"click\", () => document.getElementById(\"input-\" + id).click());\n  div.addEventListener(\"dragover\", e => { e.preventDefault(); div.classList.add(\"dragover\"); });\n  div.addEventListener(\"dragleave\", () => div.classList.remove(\"dragover\"));\n  div.addEventListener(\"drop\", e => {\n    e.preventDefault(); div.classList.remove(\"dragover\");\n    if (e.dataTransfer.files.length) handleFile(id, e.dataTransfer.files[0]);\n  });\n  document.getElementById(\"input-\" + id)?.remove;\n  container.appendChild(div);\n  div.querySelector(\"input\").addEventListener(\"change\", e => {\n    if (e.target.files.length) handleFile(id, e.target.files[0]);\n  });\n});\nasync function handleFile(slotId, file) {\n  const status = document.getElementById(\"status-\" + slotId);\n  const slot = document.getElementById(\"slot-\" + slotId);\n  if (!file.name.toLowerCase().endsWith(\".xlsx\")) {\n    status.textContent = \"⚠ .xlsx 파일만 가능\";\n    status.style.color = \"#ea4335\";\n    return;\n  }\n  status.textContent = \"업로드 중...\";\n  const reader = new FileReader();\n  reader.onload = async () => {\n    const b64 = btoa(new Uint8Array(reader.result).reduce((s,b) => s + String.fromCharCode(b), \"\"));\n    const result = await google.colab.kernel.invokeFunction(\"notebook._receive_file\", [slotId, file.name, b64], {});\n    const msg = result.data[\"text/plain\"].replace(/^'|'$/g, \"\");\n    status.textContent = \"✓ \" + msg;\n    slot.classList.add(\"done\");\n  };\n  reader.readAsArrayBuffer(file);\n}\n</script>\n\"\"\"))\nprint(\"⬆ 위 위젯에 3개의 파일을 모두 업로드한 후, 아래 셀을 실행하세요.\")"
  },
  {
   "cell_type": "code",
   "source": "missing = [info[\"label\"] for sid, info in FILE_SLOTS.items() if sid not in _uploaded]\nif missing:\n    raise FileNotFoundError(f\"업로드되지 않은 파일: {', '.join(missing)}\")\n\nfor sid, info in FILE_SLOTS.items():\n    setattr(_cfg, info[\"config\"], _uploaded[sid])\n\nbaseline_sheets = load_emr_file(_cfg.BASELINE_FILE)\ndata_sheets = load_emr_file(_cfg.INPUT_DATA_FILE)\n\nprint(f\"Baseline sheets: {list(baseline_sheets.keys())}\")\nprint(f\"Data sheets: {list(data_sheets.keys())}\")\nfor name, df in data_sheets.items():\n    print(f\"  {name}: {len(df)} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_outputs = await extract_all(data_sheets)\n",
    "\n",
    "print(f\"Extracted from {len(extractor_outputs)} sheets:\")\n",
    "for eo in extractor_outputs:\n",
    "    print(f\"  {eo.sheet_name}: {len(eo.findings)} findings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for eo in extractor_outputs:\n",
    "    out_path = EXTRACTIONS_DIR / f\"{eo.sheet_name.replace(' ', '_').lower()}.json\"\n",
    "    out_path.write_text(json.dumps(eo.model_dump(), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {eo.sheet_name: eo for eo in extractor_outputs}\n",
    "\n",
    "for sheet_name, eo in results.items():\n",
    "    source_rows = len(data_sheets[sheet_name])\n",
    "    findings_count = len(eo.findings)\n",
    "    header = (\n",
    "        f\"### {sheet_name}\\n\\n\"\n",
    "        f\"**Source rows:** {source_rows} | \"\n",
    "        f\"**Findings extracted:** {findings_count}\\n\\n\"\n",
    "    )\n",
    "    rows = []\n",
    "    for i, f in enumerate(eo.findings, 1):\n",
    "        rows.append(f\"| {i} | {f.datetime or '—'} | {f.category} | {f.content[:120]}{'…' if len(f.content) > 120 else ''} |\")\n",
    "    table = \"| # | Datetime | Category | Content |\\n|---|----------|----------|---------|\\n\" + \"\\n\".join(rows)\n",
    "    display(Markdown(header + table))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rows = []\n",
    "for sheet_name in SHEET_NAMES:\n",
    "    source_rows = len(data_sheets.get(sheet_name, pd.DataFrame()))\n",
    "    findings = len(results[sheet_name].findings) if sheet_name in results else 0\n",
    "    pct = (findings / source_rows * 100) if source_rows > 0 else 0.0\n",
    "    coverage_rows.append({\"Sheet\": sheet_name, \"Source Rows\": source_rows, \"Findings Extracted\": findings, \"Coverage %\": round(pct, 1)})\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_output = await interpret(extractor_outputs, baseline_sheets)\n",
    "\n",
    "print(f\"Reconciled findings: {len(interpreter_output.reconciled_findings)}\")\n",
    "print(f\"Conflicts resolved:  {len(interpreter_output.conflicts_resolved)}\")\n",
    "print(f\"Duplicates removed:  {interpreter_output.duplicates_removed}\")\n",
    "print(f\"Input findings:      {interpreter_output.metadata.total_input_findings}\")\n",
    "print(f\"Output findings:     {interpreter_output.metadata.total_output_findings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "interp_path = OUTPUT_DIR / \"interpretation.json\"\n",
    "interp_path.write_text(\n",
    "    json.dumps(interpreter_output.model_dump(), ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(f\"Saved: {interp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, rf in enumerate(interpreter_output.reconciled_findings, 1):\n",
    "    sources = \", \".join(rf.sources)\n",
    "    note = rf.resolution_note or \"—\"\n",
    "    content_preview = rf.content[:100] + (\"…\" if len(rf.content) > 100 else \"\")\n",
    "    rows.append(f\"| {i} | {rf.datetime or '—'} | {content_preview} | {sources} | {note} |\")\n",
    "\n",
    "table = (\n",
    "    \"| # | Datetime | Content | Sources | Resolution Note |\\n\"\n",
    "    \"|---|----------|---------|---------|--------------------|\\n\"\n",
    "    + \"\\n\".join(rows)\n",
    ")\n",
    "display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflict Resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interpreter_output.conflicts_resolved:\n",
    "    display(Markdown(\"*No conflicts detected.*\"))\n",
    "else:\n",
    "    for i, cr in enumerate(interpreter_output.conflicts_resolved, 1):\n",
    "        md = (\n",
    "            f\"### Conflict {i}\\n\\n\"\n",
    "            f\"**Description:** {cr.description}\\n\\n\"\n",
    "            f\"**Sources:** {', '.join(cr.sources)}\\n\\n\"\n",
    "            f\"**Resolution:** {cr.resolution}\"\n",
    "        )\n",
    "        display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_output = await synthesize(interpreter_output)\n",
    "\n",
    "print(f\"Findings incorporated: {synthesizer_output.metadata.findings_incorporated}\")\n",
    "print(f\"Date range: {synthesizer_output.metadata.date_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Synthesis Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_sheets = pd.read_excel(OUTPUT_FRAMEWORK_FILE, engine=\"openpyxl\", sheet_name=None)\n",
    "expected_sections = list(framework_sheets.keys())\n",
    "\n",
    "summary_text = synthesizer_output.summary.lower()\n",
    "coverage_rows = []\n",
    "for section in expected_sections:\n",
    "    found = section.lower() in summary_text\n",
    "    coverage_rows.append({\"Section\": section, \"Found in Summary\": found})\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "found_count = coverage_df[\"Found in Summary\"].sum()\n",
    "total = len(coverage_df)\n",
    "print(f\"Section coverage: {found_count}/{total} ({found_count/total*100:.0f}%)\\n\")\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(synthesizer_output.summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = OUTPUT_DIR / \"summary.md\"\n",
    "summary_path.write_text(synthesizer_output.summary, encoding=\"utf-8\")\n",
    "print(f\"Saved: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}