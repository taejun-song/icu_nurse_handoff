{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ICU Clinical Handoff Summary Pipeline\n\nEMR 데이터 로드 → 소견 추출 → 해석 (중복 제거/충돌 해소) → 검증 → 인수인계 요약 생성"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup\n\n프로젝트 루트에서 노트북을 실행하세요. 인터넷 접속이 불가한 환경에서는 사전에 의존성을 설치해야 합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys, os\nfrom pathlib import Path\n\n_nb_dir = Path(globals().get(\"_dh\", [\".\"])[0]).resolve()\nREPO_ROOT = _nb_dir.parent if _nb_dir.name == \"notebooks\" else _nb_dir\nassert (REPO_ROOT / \"src\").exists(), f\"src/ not found in {REPO_ROOT}. 프로젝트 루트를 확인하세요.\"\n\nos.chdir(REPO_ROOT)\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\nprint(f\"Working directory: {REPO_ROOT}\")\n\nimport json\nimport pandas as pd\nfrom IPython.display import Markdown, display\nfrom src.config import (\n    BASELINE_FILE, INPUT_DATA_FILE, OUTPUT_DIR, EXTRACTIONS_DIR,\n    OUTPUT_FRAMEWORK, OUTPUT_SECTIONS, SHEET_NAMES, SHEET_NAME_TO_PROMPT,\n)\nfrom src.loader import load_emr_file\nfrom src.extractors import extract_all, extract_sheet\nfrom src.interpreter import interpret\nfrom src.validator import validate\nfrom src.generator import generate\nfrom src.schemas import ExtractorOutput\nprint(\"All modules loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "hf_token = os.environ.get(\"HF_TOKEN\", \"\")\nif hf_token:\n    from huggingface_hub import login\n    login(token=hf_token)\n    print(\"Logged in to Hugging Face\")\nelse:\n    print(\"HF_TOKEN not set, skipping login\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load EMR Data\n\n아래 셀을 실행하면 파일 업로드 위젯이 나타납니다.\n2개의 Excel 파일을 선택한 뒤, 다음 셀을 실행하세요:\n- **Baseline** — 환자 기본 정보\n- **Data** — EMR 데이터"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import src.config as _cfg\n\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display as ipy_display\n    baseline_uploader = widgets.FileUpload(accept=\".xlsx\", multiple=False, description=\"Baseline\")\n    data_uploader = widgets.FileUpload(accept=\".xlsx\", multiple=False, description=\"Data\")\n    print(\"1/2) Input_Baseline 파일을 선택하세요 (환자 기본 정보):\")\n    ipy_display(baseline_uploader)\n    print(\"2/2) Input_Data 파일을 선택하세요 (EMR 데이터):\")\n    ipy_display(data_uploader)\n    _USE_WIDGET = True\nexcept ImportError:\n    print(\"ipywidgets 미설치 — 다음 셀에서 파일 경로를 직접 입력합니다.\")\n    _USE_WIDGET = False"
  },
  {
   "cell_type": "code",
   "source": "if _USE_WIDGET:\n    assert len(baseline_uploader.value) > 0, \"Baseline 파일을 먼저 업로드하세요\"\n    assert len(data_uploader.value) > 0, \"Data 파일을 먼저 업로드하세요\"\n    for item in baseline_uploader.value:\n        path = DATA_DIR / item.name\n        path.write_bytes(item.content.tobytes())\n        _cfg.BASELINE_FILE = path\n    for item in data_uploader.value:\n        path = DATA_DIR / item.name\n        path.write_bytes(item.content.tobytes())\n        _cfg.INPUT_DATA_FILE = path\nelse:\n    baseline_path = input(\"Baseline 파일 경로: \").strip()\n    data_path = input(\"Data 파일 경로: \").strip()\n    _cfg.BASELINE_FILE = Path(baseline_path)\n    _cfg.INPUT_DATA_FILE = Path(data_path)\n\nbaseline_sheets = load_emr_file(_cfg.BASELINE_FILE)\ndata_sheets = load_emr_file(_cfg.INPUT_DATA_FILE)\n\nprint(f\"\\nBaseline sheets: {list(baseline_sheets.keys())}\")\nprint(f\"Data sheets: {list(data_sheets.keys())}\")\nfor name, df in data_sheets.items():\n    print(f\"  {name}: {len(df)} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_outputs = await extract_all(data_sheets)\n",
    "\n",
    "print(f\"Extracted from {len(extractor_outputs)} sheets:\")\n",
    "for eo in extractor_outputs:\n",
    "    print(f\"  {eo.sheet_name}: {len(eo.findings)} findings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for eo in extractor_outputs:\n",
    "    out_path = EXTRACTIONS_DIR / f\"{eo.sheet_name.replace(' ', '_').lower()}.json\"\n",
    "    out_path.write_text(json.dumps(eo.model_dump(), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {eo.sheet_name: eo for eo in extractor_outputs}\n",
    "\n",
    "for sheet_name, eo in results.items():\n",
    "    source_rows = len(data_sheets[sheet_name])\n",
    "    findings_count = len(eo.findings)\n",
    "    header = (\n",
    "        f\"### {sheet_name}\\n\\n\"\n",
    "        f\"**Source rows:** {source_rows} | \"\n",
    "        f\"**Findings extracted:** {findings_count}\\n\\n\"\n",
    "    )\n",
    "    rows = []\n",
    "    for i, f in enumerate(eo.findings, 1):\n",
    "        rows.append(f\"| {i} | {f.datetime or '—'} | {f.category} | {f.content[:120]}{'…' if len(f.content) > 120 else ''} |\")\n",
    "    table = \"| # | Datetime | Category | Content |\\n|---|----------|----------|---------|\\n\" + \"\\n\".join(rows)\n",
    "    display(Markdown(header + table))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rows = []\n",
    "for sheet_name in SHEET_NAMES:\n",
    "    source_rows = len(data_sheets.get(sheet_name, pd.DataFrame()))\n",
    "    findings = len(results[sheet_name].findings) if sheet_name in results else 0\n",
    "    pct = (findings / source_rows * 100) if source_rows > 0 else 0.0\n",
    "    coverage_rows.append({\"Sheet\": sheet_name, \"Source Rows\": source_rows, \"Findings Extracted\": findings, \"Coverage %\": round(pct, 1)})\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "display(coverage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_output = await interpret(extractor_outputs, baseline_sheets)\n",
    "\n",
    "print(f\"Reconciled findings: {len(interpreter_output.reconciled_findings)}\")\n",
    "print(f\"Conflicts resolved:  {len(interpreter_output.conflicts_resolved)}\")\n",
    "print(f\"Duplicates removed:  {interpreter_output.duplicates_removed}\")\n",
    "print(f\"Input findings:      {interpreter_output.metadata.total_input_findings}\")\n",
    "print(f\"Output findings:     {interpreter_output.metadata.total_output_findings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "interp_path = OUTPUT_DIR / \"interpretation.json\"\n",
    "interp_path.write_text(\n",
    "    json.dumps(interpreter_output.model_dump(), ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(f\"Saved: {interp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, rf in enumerate(interpreter_output.reconciled_findings, 1):\n",
    "    sources = \", \".join(rf.sources)\n",
    "    note = rf.resolution_note or \"—\"\n",
    "    content_preview = rf.content[:100] + (\"…\" if len(rf.content) > 100 else \"\")\n",
    "    rows.append(f\"| {i} | {rf.datetime or '—'} | {content_preview} | {sources} | {note} |\")\n",
    "\n",
    "table = (\n",
    "    \"| # | Datetime | Content | Sources | Resolution Note |\\n\"\n",
    "    \"|---|----------|---------|---------|--------------------|\\n\"\n",
    "    + \"\\n\".join(rows)\n",
    ")\n",
    "display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflict Resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interpreter_output.conflicts_resolved:\n",
    "    display(Markdown(\"*No conflicts detected.*\"))\n",
    "else:\n",
    "    for i, cr in enumerate(interpreter_output.conflicts_resolved, 1):\n",
    "        md = (\n",
    "            f\"### Conflict {i}\\n\\n\"\n",
    "            f\"**Description:** {cr.description}\\n\\n\"\n",
    "            f\"**Sources:** {', '.join(cr.sources)}\\n\\n\"\n",
    "            f\"**Resolution:** {cr.resolution}\"\n",
    "        )\n",
    "        display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Run Validator"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "validator_output = await validate(interpreter_output, baseline_sheets)\n\nprint(f\"Validated findings:    {len(validator_output.validated_findings)}\")\nprint(f\"Missing findings:      {len(validator_output.missing_findings)}\")\nprint(f\"Unresolved conflicts:  {len(validator_output.unresolved_conflicts)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Validation Inspection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nval_path = OUTPUT_DIR / \"validation.json\"\nval_path.write_text(\n    json.dumps(validator_output.model_dump(), ensure_ascii=False, indent=2),\n    encoding=\"utf-8\",\n)\nprint(f\"Saved: {val_path}\")\n\nif validator_output.missing_findings:\n    display(Markdown(\"### Missing Findings\"))\n    for mf in validator_output.missing_findings:\n        display(Markdown(f\"- {mf}\"))\nelse:\n    display(Markdown(\"*No missing findings detected.*\"))\n\nif validator_output.unresolved_conflicts:\n    display(Markdown(\"### Unresolved Conflicts\"))\n    for uc in validator_output.unresolved_conflicts:\n        display(Markdown(f\"- {uc}\"))\nelse:\n    display(Markdown(\"*No unresolved conflicts.*\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Run Generator"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "generator_output = await generate(validator_output)\nprint(f\"Summary length: {len(generator_output.summary)} chars\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "expected_sections = list(dict.fromkeys(OUTPUT_SECTIONS))\nsummary_text = generator_output.summary.lower()\ncoverage_rows = []\nfor section in expected_sections:\n    found = section.lower() in summary_text\n    coverage_rows.append({\"Section\": section, \"Found in Summary\": found})\n\ncoverage_df = pd.DataFrame(coverage_rows)\nfound_count = coverage_df[\"Found in Summary\"].sum()\ntotal = len(coverage_df)\nprint(f\"Section coverage: {found_count}/{total} ({found_count/total*100:.0f}%)\\n\")\ndisplay(coverage_df)"
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Final Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "display(Markdown(generator_output.summary))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "summary_path = OUTPUT_DIR / \"summary.md\"\nsummary_path.write_text(generator_output.summary, encoding=\"utf-8\")\nprint(f\"Saved: {summary_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}